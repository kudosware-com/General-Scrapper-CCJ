{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import bs4\n",
    "import datetime\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "from pytesseract import image_to_string \n",
    "from PIL import Image\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import threading\n",
    "from multiprocessing import Process\n",
    "\n",
    "\n",
    "# get captcha text from screenshot\n",
    "def get_captcha_text(location, size,state):\n",
    "    pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "    im = Image.open('{link}.png'.format(link=state)) # uses PIL library to open image in memory\n",
    "    \n",
    "    # adjust the co-ordinates of captcha image from screenshot\n",
    "    left = location['x'] + 90\n",
    "    top = location['y'] + 67\n",
    "    right = location['x'] + size['width'] + 90\n",
    "    bottom = location['y'] + size['height'] + 67\n",
    "\n",
    "\n",
    "    im = im.crop((left, top, right, bottom)) # defines crop points\n",
    "    im.save('{link}.png'.format(link=state))\n",
    "    # getting text from image\n",
    "    captcha_text = image_to_string(Image.open('{link}.png'.format(link=state)))\n",
    "    return captcha_text\n",
    "\n",
    "# download pdf from url\n",
    "def pdfdownload(url,filename,cookie):\n",
    "    \n",
    "    # Add cookie for authenticating the request\n",
    "    requestsJar = requests.cookies.RequestsCookieJar()\n",
    "    requestsJar.set(\"PHPSESSID\",cookie)\n",
    "    \n",
    "    response = requests.get(url,cookies=requestsJar)\n",
    "    \n",
    "    # create pdf file and dump the data from url\n",
    "    file = open(\"D:/Internship/pdffiles/PdfDocument Scarping2/\" + filename + \".pdf\", 'wb')\n",
    "    file.write(response.content)\n",
    "    file.close()\n",
    "\n",
    "def login_to_website(state,FromDate,ToDate,url):\n",
    "    driver = webdriver.Chrome(executable_path = \"D:/Internship/chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    \n",
    "    # find part of the page you want image of\n",
    "    element = driver.find_element_by_id('captcha_image') \n",
    "    location = element.location\n",
    "    size = element.size\n",
    "    driver.save_screenshot('D:/Internship/My scrapper/{part}.png'.format(part=state))\n",
    "    \n",
    "    #storing cookies for furthur downloads\n",
    "    cookies = driver.get_cookies()\n",
    "    driver.find_element_by_id(\"to_date\").send_keys(ToDate)\n",
    "    driver.find_element_by_id(\"from_date\").send_keys(FromDate)\n",
    "    captcha = driver.find_element_by_id('captcha_image')\n",
    "    \n",
    "    # get captcha text\n",
    "    captcha_text = get_captcha_text(location, size,state)\n",
    "    captcha.send_keys(captcha_text)\n",
    "    print(captcha_text)\n",
    "    driver.find_element_by_id(\"captcha\").send_keys(captcha_text)\n",
    "    sleep(5)\n",
    "    \n",
    "    #clicking the button\n",
    "    driver.find_element_by_xpath(\"/html/body/form/div[2]/div[4]/span[3]/input[1]\").click()    \n",
    "    \n",
    "    sleep(5)\n",
    "    page_source = driver.page_source\n",
    "    soup1 = BeautifulSoup(page_source, 'lxml')\n",
    "    type(soup1)\n",
    "    bs4.BeautifulSoup\n",
    "    title = soup1.title\n",
    "    case=[]\n",
    "    CaseType_CaseNumber_CaseYear=[]\n",
    "    Order_Date=[]\n",
    "    Order_Number=[]\n",
    "    status=[]\n",
    "    \n",
    "    # getting table of all pdfs\n",
    "    PDF_file_Table_tag=soup1.find_all(\"table\",{\"id\":\"showList3\"})[0]\n",
    "\n",
    "    tr_tag=PDF_file_Table_tag.find_all(\"tr\")\n",
    "    print(len(tr_tag))\n",
    "\n",
    "    for i in range(2,len(tr_tag)):\n",
    "        td_tag=tr_tag[i].find_all(\"td\") [0] \n",
    "        td_tag1=tr_tag[i].find_all(\"td\")[1]\n",
    "        td_tag2=tr_tag[i].find_all(\"td\")[2]\n",
    "        td_tag3=tr_tag[i].find_all(\"td\")[3]\n",
    "        \n",
    "\n",
    "        case.append(td_tag.text)\n",
    "        CaseType_CaseNumber_CaseYear.append(td_tag1.text)\n",
    "        Order_Date.append(td_tag2.text)\n",
    "        Order_Number.append(td_tag3.text)\n",
    "        \n",
    "        \n",
    "        document=td_tag1.text\n",
    "        print(document)\n",
    "        try:\n",
    "            # try going into link\n",
    "            a_tag_url=td_tag3.find(\"a\")[\"href\"]\n",
    "            status.append(a_tag_url)\n",
    "            print(str(i-1)+\")\"+a_tag_url)\n",
    "            \n",
    "            # download each file\n",
    "            pdfdownload(\"https://services.ecourts.gov.in/ecourtindiaHC/cases/\"+ a_tag_url,document.replace(\"/\",\" \"),cookies[0][\"value\"])\n",
    "\n",
    "            \n",
    "        except:\n",
    "            Order_Number.append(\"NaN\")\n",
    "            print(\"No Document\")\n",
    "            \n",
    "    a={\"case\":case,\"CaseType_CaseNumber_CaseYear\":CaseType_CaseNumber_CaseYear,\"Order_Date\":Order_Date,\"Order_Number\":Order_Number,\"statulink\":status}\n",
    "            \n",
    "    df = pd.DataFrame.from_dict(a, orient='index')\n",
    "    df = df.transpose()\n",
    "    \n",
    "    # store details of each case in CSV file\n",
    "    df.to_csv(r\"D:\\Internship\\pdffiles\\csvfiles\\courtt.csv\", sep=',',index=False)\n",
    "    df.head(50)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allStates():\n",
    "    driver = webdriver.Chrome(executable_path = \"D:/Internship/chromedriver.exe\")\n",
    "    driver.get(\"https://services.ecourts.gov.in/ecourtindiaHC/\")\n",
    "    \n",
    "    # getting list of all the states with one court\n",
    "    stateList = driver.find_elements_by_xpath(\"/html/body/div/ul/li/a\")\n",
    "    stateLinks = list()\n",
    "    stateName = list()\n",
    "    for state in stateList:\n",
    "        stateName.append(state.text)\n",
    "        stateLinks.append(state.get_attribute(\"href\"))\n",
    "    \n",
    "    # getting list of all the states with multiple court\n",
    "    stateList = driver.find_elements_by_xpath(\"/html/body/div/ul/li/ul/li/a\")\n",
    "    for state in stateList:\n",
    "        stateName.append(state.text)\n",
    "        stateLinks.append(state.get_attribute(\"href\"))\n",
    "    \n",
    "    byDate = list()\n",
    "    \n",
    "    # creating list of links for cases by date for each court\n",
    "    for link in stateLinks:\n",
    "        driver.get(link)\n",
    "        try:\n",
    "            item = driver.find_element_by_xpath(\"/html/body/div[3]/ul/li[2]/ol/li[5]/a\")\n",
    "        except:\n",
    "            item = driver.find_element_by_xpath(\"/html/body/div/ul/li[2]/ol/li[5]/a\")\n",
    "        byDate.append(item.get_attribute(\"href\"))\n",
    "    \n",
    "    # visit each court site and get all the cases for current date\n",
    "    current_date = datetime.datetime.now()\n",
    "    processes = []\n",
    "    count = 0\n",
    "    for i in range(len(byDate)):\n",
    "#         t = threading.Thread(target=login_to_website,args=(stateName[i],current_date.strftime(\"%d-%m-%Y\"),current_date.strftime(\"%d-%m-%Y\"),byDate[i]))\n",
    "        p = Process(target=login_to_website,args=(stateName[i],\"30-6-2021\",\"30-6-2021\",byDate[i]))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "        count += 1;\n",
    "        if(count > 2):\n",
    "            break;\n",
    "    \n",
    "    for t in processes:\n",
    "        t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    allStates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
